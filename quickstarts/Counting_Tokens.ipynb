{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAtdeyo2ztHO",
        "outputId": "abfd7e6a-c1c9-461a-b0c6-c03d32edeed4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "umH3IXyYz9AP",
        "outputId": "3374f93f-4b55-4f21-ef90-522ac8eec41d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting flashtext<3.0,>=2.7 (from hazm)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim<5.0.0,>=4.3.1 (from hazm)\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy==1.24.3 (from hazm)\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from hazm) (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.2.0)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.3.0.post1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.2)\n",
            "Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=3f8065dcaad7bb638df3547c4c1d217f1282e18e8799fac68977035510d0a775\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/20/47/f03dfa8a7239c54cbc44ff7389eefbf888d2c1873edaaec888\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, python-crfsuite, pybind11, numpy, scipy, fasttext-wheel, gensim, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 gensim-4.3.3 hazm-0.10.0 numpy-1.24.3 pybind11-3.0.0 python-crfsuite-0.9.11 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f9fc9886b573401f9d0045028baf3a24"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from hazm import Normalizer, SentenceTokenizer, WordTokenizer\n",
        "import re\n",
        "import nltk\n",
        "from tqdm.notebook import tqdm # For progress bar\n",
        "\n",
        "# Download NLTK data (punkt for sentence tokenizer)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Hazm tools\n",
        "normalizer = Normalizer()\n",
        "sentence_tokenizer = SentenceTokenizer()\n",
        "word_tokenizer = WordTokenizer()\n",
        "\n",
        "# Function to clean and normalize text\n",
        "def clean_text(text):\n",
        "    text = normalizer.normalize(text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Function to preprocess a single legal text file\n",
        "def preprocess_legal_text_initial(file_path, output_dir):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        cleaned_text = clean_text(text)\n",
        "        sentences = sentence_tokenizer.tokenize(cleaned_text)\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Create output file path\n",
        "        file_name = os.path.basename(file_path)\n",
        "        output_file_path = os.path.join(output_dir, f\"processed_{file_name}\")\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip(): # Write non-empty sentences\n",
        "                    f_out.write(sentence.strip() + '\\n')\n",
        "\n",
        "        print(f\"Successfully processed {file_name} and saved to {output_file_path}\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "ko6jEYbU0BxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your input directory for raw legal texts\n",
        "input_legal_texts_dir = '/content/drive/MyDrive/my_legal_corpus' # ***** این مسیر رو با پوشه فایل‌های خام خودت اصلاح کن *****\n",
        "\n",
        "# Define the output directory for initially processed text files\n",
        "processed_output_dir = '/content/drive/MyDrive/processed_legal_texts_temp'\n",
        "os.makedirs(processed_output_dir, exist_ok=True)\n",
        "\n",
        "# Process all files in the input directory\n",
        "print(f\"Starting initial preprocessing of files from: {input_legal_texts_dir}\")\n",
        "processed_files_count = 0\n",
        "for filename in tqdm(os.listdir(input_legal_texts_dir), desc=\"Processing files\"):\n",
        "    if filename.endswith(\".txt\"): # Process only .txt files\n",
        "        file_path = os.path.join(input_legal_texts_dir, filename)\n",
        "        if preprocess_legal_text_initial(file_path, processed_output_dir):\n",
        "            processed_files_count += 1\n",
        "\n",
        "print(f\"\\nFinished initial preprocessing. Total files processed: {processed_files_count}\")"
      ],
      "metadata": {
        "id": "r1n690bv0DMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the final combined text file\n",
        "final_combined_text_file_path = '/content/drive/MyDrive/all_legal_sentences.txt'\n",
        "\n",
        "print(f\"Combining all processed sentences into: {final_combined_text_file_path}\")\n",
        "total_lines = 0\n",
        "with open(final_combined_text_file_path, 'w', encoding='utf-8') as outfile:\n",
        "    for filename in tqdm(os.listdir(processed_output_dir), desc=\"Combining files\"):\n",
        "        if filename.startswith(\"processed_\") and filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(processed_output_dir, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as infile:\n",
        "                for line in infile:\n",
        "                    if line.strip(): # Write only non-empty lines\n",
        "                        outfile.write(line)\n",
        "                        total_lines += 1\n",
        "\n",
        "print(f\"\\nAll processed sentences combined. Total lines written: {total_lines}\")\n",
        "print(\"Initial preprocessing and data consolidation complete!\")"
      ],
      "metadata": {
        "id": "kAWdAett0tC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the final combined text file\n",
        "final_combined_text_file_path = '/content/drive/MyDrive/all_legal_sentences.txt'\n",
        "\n",
        "print(f\"Combining all processed sentences into: {final_combined_text_file_path}\")\n",
        "total_lines = 0\n",
        "with open(final_combined_text_file_path, 'w', encoding='utf-8') as outfile:\n",
        "    for filename in tqdm(os.listdir(processed_output_dir), desc=\"Combining files\"):\n",
        "        if filename.startswith(\"processed_\") and filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(processed_output_dir, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as infile:\n",
        "                for line in infile:\n",
        "                    if line.strip(): # Write only non-empty lines\n",
        "                        outfile.write(line)\n",
        "                        total_lines += 1\n",
        "\n",
        "print(f\"\\nAll processed sentences combined. Total lines written: {total_lines}\")\n",
        "print(\"Initial preprocessing and data consolidation complete!\")"
      ],
      "metadata": {
        "id": "H6DhdvKH1XpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate tokenizers bitsandbytes\n",
        "\n",
        "# بعد از اجرای این سلول، نیازی به ریستارت Runtime نیست چون محیط کاملاً تازه است و GPU فعال.\n",
        "print(\"\\nRequired Hugging Face libraries installed.\")"
      ],
      "metadata": {
        "id": "xm6cRoGp1g4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "from pathlib import Path\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the path to your combined text file in Google Drive (created in Cell 5)\n",
        "text_file_path = '/content/drive/MyDrive/all_legal_sentences.txt'\n",
        "\n",
        "# Ensure the directory for saving the tokenizer exists\n",
        "tokenizer_output_dir = '/content/drive/MyDrive/custom_legal_bert_tokenizer'\n",
        "os.makedirs(tokenizer_output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=False,\n",
        ")\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train(\n",
        "    files=[text_file_path],\n",
        "    vocab_size=30000, # You can adjust this based on your corpus size\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        ")\n",
        "\n",
        "# Save the tokenizer files\n",
        "tokenizer.save_model(tokenizer_output_dir)\n",
        "\n",
        "print(f\"\\nTokenizer training complete! Files saved to: {tokenizer_output_dir}\")\n",
        "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "# Optional: Load and test the tokenizer\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(tokenizer_output_dir)\n",
        "test_sentence = \"این یک نمونه متن حقوقی برای تست توکنایزر است. ماده ۱۲ قانون مجازات اسلامی.\"\n",
        "encoded = loaded_tokenizer.encode_plus(test_sentence, add_special_tokens=True)\n",
        "print(f\"\\nTest sentence: {test_sentence}\")\n",
        "print(f\"Encoded IDs: {encoded.input_ids}\")\n",
        "print(f\"Decoded tokens: {loaded_tokenizer.convert_ids_to_tokens(encoded.input_ids)}\")"
      ],
      "metadata": {
        "id": "uU0rXmmY1lUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import os\n",
        "\n",
        "# Define the path where your custom tokenizer was saved (created in Cell 7)\n",
        "tokenizer_output_dir = '/content/drive/MyDrive/custom_legal_bert_tokenizer'\n",
        "\n",
        "# Define the path to your combined text file in Google Drive (created in Cell 5)\n",
        "text_file_path = '/content/drive/MyDrive/all_legal_sentences.txt'\n",
        "\n",
        "# 1. Load the custom tokenizer\n",
        "print(\"Loading custom tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_output_dir)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "# 2. Load the dataset (Revised approach for reading text file directly)\n",
        "print(f\"Loading dataset from: {text_file_path}\")\n",
        "try:\n",
        "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    raw_dataset = Dataset.from_dict({\"text\": lines})\n",
        "    dataset = DatasetDict({\"train\": raw_dataset})\n",
        "\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Dataset structure: {dataset}\")\n",
        "\n",
        "    print(\"\\nSample from dataset:\")\n",
        "    print(dataset[\"train\"][0])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{text_file_path}' was not found. Please ensure the path is correct and the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the dataset: {e}\")"
      ],
      "metadata": {
        "id": "W3zjpCk31l5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Truncate to maximum sequence length if sentences are too long\n",
        "    # We will use the tokenizer's model_max_length which is typically 512 for BERT\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=os.cpu_count(), # Use multiple processes if available for faster tokenization\n",
        "    remove_columns=[\"text\"],\n",
        ")\n",
        "print(\"Dataset tokenized successfully.\")\n",
        "print(f\"Tokenized dataset structure: {tokenized_datasets}\")\n",
        "\n",
        "\n",
        "# Function to group texts into blocks of max_length for MLM\n",
        "block_size = 128 # You can increase this to 256 or 512 if your GPU memory allows and you want longer contexts\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "print(f\"Grouping texts into blocks of size {block_size}...\")\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=os.cpu_count(),\n",
        ")\n",
        "print(\"Text grouping complete.\")\n",
        "print(f\"Final dataset structure for MLM: {lm_datasets}\")\n",
        "\n",
        "print(\"\\nSample of prepared MLM dataset (first example):\")\n",
        "print(lm_datasets[\"train\"][0])"
      ],
      "metadata": {
        "id": "Uv6QekV41mSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 3. Configure the BERT model from scratch\n",
        "# This creates a new BERT model with the specified configuration.\n",
        "print(\"Initializing BERT configuration and model...\")\n",
        "config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size, # Use the vocab size from your trained tokenizer\n",
        "    max_position_embeddings=512, # Max sequence length for the model\n",
        "    num_hidden_layers=6, # Number of transformer layers (you can increase this to 12 for base BERT if GPU allows)\n",
        "    num_attention_heads=6, # Number of attention heads (you can increase this to 12 for base BERT if GPU allows)\n",
        "    hidden_size=384, # Dimension of the hidden layers (you can increase this to 768 for base BERT if GPU allows)\n",
        "    type_vocab_size=2,\n",
        ")\n",
        "\n",
        "model = BertForMaskedLM(config=config)\n",
        "print(f\"Number of parameters in the new model: {model.num_parameters()}\")\n",
        "\n",
        "\n",
        "# 4. Set up Training Arguments\n",
        "output_dir = '/content/drive/MyDrive/legal_bert_pretraining_output' # Directory to save checkpoints and logs\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Setting up Training Arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5, # Number of training epochs (iterations over the dataset) - Adjust based on corpus size and convergence\n",
        "    per_device_train_batch_size=32, # Batch size per GPU/CPU - Adjust based on GPU memory\n",
        "    save_steps=10_000, # Save model checkpoint every 10,000 steps\n",
        "    save_total_limit=2, # Keep only the last 2 checkpoints\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=100, # Log training progress every 100 steps\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    do_train=True,\n",
        "    gradient_accumulation_steps=1, # Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    # Optionally enable mixed precision training for faster training and less memory usage (if using GPU)\n",
        "    # fp16=True, # For NVIDIA GPUs\n",
        "    # bf16=True, # For AMD GPUs and newer NVIDIA GPUs (e.g., A100)\n",
        ")\n",
        "\n",
        "\n",
        "# Data collator for Masked Language Modeling\n",
        "print(\"Setting up Data Collator for Masked Language Modeling...\")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15 # 15% of tokens will be masked for prediction\n",
        ")\n",
        "\n",
        "# 5. Initialize the Trainer and start pre-training\n",
        "print(\"Initializing Trainer and starting pre-training...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting BERT Pre-training ---\")\n",
        "# Start training! This will take a long time depending on your dataset size and GPU.\n",
        "trainer.train()\n",
        "print(\"\\nBERT Pre-training completed!\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_save_path = os.path.join(output_dir, \"final_model\")\n",
        "model.save_pretrained(final_model_save_path)\n",
        "tokenizer.save_pretrained(final_model_save_path) # Save tokenizer with the model\n",
        "print(f\"\\nFinal pre-trained model and tokenizer saved to: {final_model_save_path}\")"
      ],
      "metadata": {
        "id": "YspWwPRP1mpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Counting_Tokens.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}